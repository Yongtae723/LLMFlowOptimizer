{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is colab tutorial notebook for LLMFlowOptimizer\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Yongtae723/LLMFlowOptimizer/blob/main/notebooks/LLMFlowOptimizer_tutorial_notebook.ipynb)\n",
    "\n",
    "LLMFlowOptimizer is made for treating component of LLMFlow as parameter and optimizing the parameters.\n",
    "\n",
    "\n",
    "![concept_image](https://github.com/Yongtae723/LLMFlowOptimizer/blob/main/documents/image/concept.png?raw=true)\n",
    "\n",
    "We assume LLMFlowOptimizer is used as repository scale, but you can experience flow and concept of LLMFlowOptimizer in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone git repo\n",
    "!git clone https://github.com/Yongtae723/LLMFlowOptimizer.git\n",
    "%cd LLMFlowOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "!pip install poetry\n",
    "!poetry config virtualenvs.in-project true\n",
    "!poetry install --no-ansi\n",
    "\n",
    "import sys\n",
    "\n",
    "VENV_PATH = \"/content/LLMFlowOptimizer/.venv/lib/python3.10/site-packages\"\n",
    "sys.path.insert(0, VENV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace dummy values with your own\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"dummy\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Model definition\n",
    "This section corresponds to the [Step 1 : Define model architect and config.](https://github.com/Yongtae723/LLMFlowOptimizer#step-1--define-model-architect-and-config) in the README.\n",
    "\n",
    "In first, you have to create class which specify your model structure.\n",
    "the code below is an example of model definition, But you can edit it as you like.\n",
    "\n",
    "Note that arguments of `__init__` can be treated as hyperparameter adn optimized afterword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.indexes.vectorstore import VectorstoreIndexCreator\n",
    "from langchain.schema.embeddings import Embeddings\n",
    "from langchain.schema.language_model import BaseLanguageModel\n",
    "from langchain.text_splitter import TextSplitter\n",
    "\n",
    "\n",
    "class SampleQA:\n",
    "    \"\"\"Define the flow of the model to be adjusted.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str,\n",
    "        embedding: Embeddings,\n",
    "        text_splitter: TextSplitter,\n",
    "        llm: BaseLanguageModel,\n",
    "    ) -> None:\n",
    "        \"\"\"Input the elements necessary for LLM flow The arguments here will be used as a\n",
    "        hyperparameters and optimized.\n",
    "\n",
    "        the arguments are defined by `configs/model/sample.yaml`\n",
    "        \"\"\"\n",
    "        self.embedding = embedding\n",
    "        self.text_splitter = text_splitter\n",
    "        self.text_loader = TextLoader(data_path)\n",
    "        self.llm = llm\n",
    "        self.index = VectorstoreIndexCreator(\n",
    "            embedding=self.embedding, text_splitter=self.text_splitter\n",
    "        ).from_loaders([self.text_loader])\n",
    "\n",
    "        self.chain = RetrievalQA.from_chain_type(\n",
    "            self.llm,\n",
    "            retriever=self.index.vectorstore.as_retriever(),\n",
    "            return_source_documents=True,\n",
    "        )\n",
    "\n",
    "    def __call__(self, question: str) -> str:\n",
    "        \"\"\"Answer the question.\"\"\"\n",
    "        return self.chain(question)\n",
    "\n",
    "    def get_chain(self) -> Chain:\n",
    "        \"\"\"Get langchain chain.\"\"\"\n",
    "        return self.chain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined component by python\n",
    "To understand easily, let's start with actual python code and see what happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "data_path = \"data/reference/nyc_wikipedia.txt\"\n",
    "embedding = OpenAIEmbeddings()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "model_class = SampleQA(\n",
    "    data_path=data_path,\n",
    "    embedding=embedding,\n",
    "    text_splitter=text_splitter,\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "print(model_class(\"What is the population of New York City?\")[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the model\n",
    "print(f\"Embedding: {model_class.embedding.__class__.__name__}\")\n",
    "print(f\"LLM: {model_class.llm.__class__.__name__}\")\n",
    "print(f\"Text Splitter: {model_class.text_splitter.__class__.__name__}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define component by yaml.\n",
    "you succeeded? Awesome! \n",
    "Then let's do the same thing with yaml file.\n",
    "\n",
    "First, please copy and pased your model into `/content/LLMFlowOptimizer/llmflowoptimizer/component/model/sample_qa.py`\n",
    "\n",
    "Then, you can load the class by Hydra based on the information of `/content/LLMFlowOptimizer/configs/model/default.yaml`.\n",
    "\n",
    "default yaml file is like below.\n",
    "```yaml\n",
    "defaults:\n",
    "  - _self_\n",
    "  - embedding: OpenAI\n",
    "  - text_splitter: RecursiveCharacter\n",
    "  - llm: OpenAI\n",
    "\n",
    "_target_: llmflowoptimizer.component.model.sample_qa.SampleQA\n",
    "\n",
    "data_path: ${paths.reference_data_dir}/nyc_wikipedia.txt\n",
    "\n",
    "```\n",
    "\n",
    "This means that you can load `llmflowoptimizer.component.model.sample_qa.SampleQA` class, and component of `__init__` are defined the same folder.\n",
    "\n",
    "For example, llm is defined in `/content/LLMFlowOptimizer/configs/model/llm/GPTTurbo.yaml`\n",
    "\n",
    "```yaml\n",
    "_target_: langchain.chat_models.ChatOpenAI\n",
    "model_name: gpt-3.5-turbo\n",
    "temperature: 0\n",
    "```\n",
    "\n",
    "that means you will load `langchain.chat_models.ChatOpenAI` class with arguments `model_name` and `temperature`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "import rootutils\n",
    "from hydra import compose, initialize\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "from omegaconf import open_dict\n",
    "\n",
    "\n",
    "def load_hydra_config():\n",
    "    with initialize(version_base=\"1.3\", config_path=\"configs\"):\n",
    "        cfg = compose(config_name=\"run.yaml\", return_hydra_config=True, overrides=[])\n",
    "        with open_dict(cfg):\n",
    "            cfg.paths.root = str(rootutils.find_root(indicator=\".project-root\"))\n",
    "    HydraConfig().set_config(cfg)\n",
    "    return cfg\n",
    "\n",
    "\n",
    "cfg = load_hydra_config()\n",
    "for key, value in cfg.model.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class = hydra.utils.instantiate(cfg.model)\n",
    "print(model_class(\"What is the population of New York City?\")[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the model\n",
    "print(f\"Embedding: {model_class.embedding.__class__.__name__}\")\n",
    "print(f\"LLM: {model_class.llm.__class__.__name__}\")\n",
    "print(f\"Text Splitter: {model_class.text_splitter.__class__.__name__}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Define evaluation \n",
    "After you define your own model, then create evaluation class.\n",
    "\n",
    "Return value of `evaluate` is used as score and component will be optimized to maximize/minimize the score.\n",
    "And method of `evaluate` use model_class which you defined in previous section.\n",
    "\n",
    "\n",
    "Following cell is example of evaluation class, which is written in `/content/LLMFlowOptimizer/llmflowoptimizer/component/evaluation/sample.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any\n",
    "\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy\n",
    "\n",
    "\n",
    "class Evaluation:\n",
    "    \"\"\"Define the evaluation system.\n",
    "\n",
    "    llmflowoptimizer optimizes the hyperparameters of the model\n",
    "    Return value of `__call__` is used as score and component will be optimized to maximize/minimize the score.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_dataset_path: str,\n",
    "    ):\n",
    "        with open(eval_dataset_path) as f:\n",
    "            self.eval_data = json.load(f)\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        model: Any,  # this model should be defined in llmflowoptimizer/component/model/sample_qa.py\n",
    "    ):\n",
    "        # simple evaluation using ragas\n",
    "        evaluation_dataset = {\n",
    "            \"question\": [],\n",
    "            \"answer\": [],\n",
    "            \"contexts\": [],\n",
    "            \"ground_truths\": [],\n",
    "        }\n",
    "        for data in self.eval_data:\n",
    "            output = model(data[\"question\"])\n",
    "            evaluation_dataset[\"question\"].append(data[\"question\"])\n",
    "            evaluation_dataset[\"answer\"].append(output[\"result\"])\n",
    "            evaluation_dataset[\"contexts\"].append(\n",
    "                [document.page_content for document in output[\"source_documents\"]]\n",
    "            )\n",
    "            evaluation_dataset[\"ground_truths\"].append([data[\"ground_truth\"]])\n",
    "        evaluation_dataset = Dataset.from_dict(evaluation_dataset)\n",
    "\n",
    "        result = evaluate(evaluation_dataset, metrics=[answer_relevancy])\n",
    "\n",
    "        return result[\"answer_relevancy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset_path = \"data/evaluation/NY_eval_data.json\"\n",
    "evaluator = Evaluation(eval_dataset_path=eval_dataset_path)\n",
    "res = evaluator.evaluate(model_class)\n",
    "\n",
    "print(f\"Ragas Score: {res}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also load this evaluation model by yaml file.\n",
    "\n",
    "Please save evaluator on `/content/LLMFlowOptimizer/llmflowoptimizer/component/evaluation/sample.py`, then you can specify the evaluator by yaml file.\n",
    "Default yaml file is like below.\n",
    "\n",
    "```yaml\n",
    "_target_: llmflowoptimizer.component.evaluation.sample.Evaluation\n",
    "eval_dataset_path: ${paths.evaluation_data_dir}/NY_eval_data.json\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_hydra_config()\n",
    "evaluator = hydra.utils.instantiate(cfg.evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = evaluator.evaluate(model_class)\n",
    "\n",
    "print(f\"Score: {res}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check model build and evaluation flow by following command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry run python llmflowoptimizer/run.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Hyperparameter optimization\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please add yaml that you want to add in hyperparameter search on each folder of `/content/LLMFlowOptimizer/configs/model/`\n",
    "\n",
    "Then you can specify search range on yaml file.\n",
    "example of yaml file is like below.\n",
    "\n",
    "```yaml\n",
    "model/text_splitter: choice(RecursiveCharacter, CharacterTextSplitter)\n",
    "model.text_splitter.chunk_size: range(500, 1500, 100)\n",
    "model/llm: choice(OpenAI, GPTTurbo, GPT4)\n",
    "```\n",
    "\n",
    "This example is a part of `configs/hparams_search/optuna.yaml` , and it means this system will search best hyperparameter from RecursiveCharacter or CharacterTextSplitter for model.text_splitter component, chunk_size is between 500 and 1500, and OpenAI, GPTTurbo, GPT4 for model.llm component.\n",
    "\n",
    "Also complicated search range can be defined by python like configs/hparams_search/custom-search-space-objective.py\n",
    "\n",
    "You can start hyperparameter search by following command.\n",
    "\n",
    "```bash\n",
    "poetry run python llmflowoptimizer/run.py hparams_search=optuna\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry run python llmflowoptimizer/run.py hparams_search=optuna"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can see the best parameter on `logs/initial_task/multirruns/{timestamp}/optimization_results.yaml`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If you like this projectðŸ’–\n",
    "\n",
    "if you like this project, you can use [this project](https://github.com/Yongtae723/LLMFlowOptimizer/blob/main/notebooks/tutorial_notebook.ipynb) as a template for your own project.\n",
    "Push `Use this template` button on the top of [this repo](https://github.com/Yongtae723/LLMFlowOptimizer/blob/main/notebooks/tutorial_notebook.ipynb), then you can create your own project based on this project."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
